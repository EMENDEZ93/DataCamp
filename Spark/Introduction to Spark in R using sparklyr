Q1:-
Load the sparklyr package with library().
Connect to Spark by calling spark_connect(), with argument master = "local". Assign the result to spark_conn.
Get the Spark version using spark_version(), with argument sc = spark_conn.
Disconnect from Spark using spark_disconnect(), with argument sc = spark_conn.

Solution:-
# Load sparklyr
library(sparklyr)

# Connect to your Spark cluster
spark_conn <- spark_connect(master="local")

# Print the version of Spark
print(spark_version(sc=spark_conn))

# Disconnect from Spark
spark_disconnect(sc=spark_conn)

Q2:-
track_metadata, containing the song name, artist name, and other metadata for 1,000 tracks, has been pre-defined in your workspace.

Use str() to explore the track_metadata dataset.
Connect to your local Spark cluster, storing the connection in spark_conn.
Copy track_metadata to the Spark cluster using copy_to() .
See which data frames are available in Spark, using src_tbls().
Disconnect from Spark.

Solution:-
# Load dplyr
library(dplyr)

# Explore track_metadata structure
str(track_metadata)

# Connect to your Spark cluster
spark_conn <- spark_connect("local")

# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn,track_metadata,overwrite=TRUE)

# List the data frames available in Spark
src_tbls(spark_conn)

# Disconnect from Spark
spark_disconnect(spark_conn)

Q3:-
A Spark connection has been created for you as spark_conn. The track metadata for 1,000 tracks is stored in the Spark cluster in the table "track_metadata".

Link to the "track_metadata" table using tbl(). Assign the result to track_metadata_tbl.
See how big the dataset is, using dim() on track_metadata_tbl.
See how small the tibble is, using object_size() on track_metadata_tbl.

Solution:-
# Link to the track_metadata table in Spark
track_metadata_tbl <- tbl(spark_conn, "track_metadata")

# See how big the dataset is
dim(track_metadata_tbl)

# See how small the tibble is
object_size(track_metadata_tbl)

Q4:-
A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.

Print the first 5 rows and all the columns of the track metadata.
Examine the structure of the tibble using str().
Examine the structure of the track metadata using glimpse().

Solution:-
# Print 5 rows, all columns
print(track_metadata_tbl,n=5,width=Inf)

# Examine structure of tibble
str(track_metadata_tbl)

# Examine structure of data
glimpse(track_metadata_tbl)

Q5:-
A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.

Select the artist_name, release, title, and year using select().
Try to do the same thing using square bracket indexing. Spoiler! This code throws an error, so it is wrapped in a call to tryCatch().

Solution:-
# track_metadata_tbl has been pre-defined
track_metadata_tbl

# Manipulate the track metadata
track_metadata_tbl %>%
  # Select columns
  select('artist_name', 'release', 'title', 'year')

# Try to select columns using [ ]
tryCatch({
    # Selection code here
    track_metadata_tbl[, c("artist_name", "release", "title", "year")]
  },
  error = print
)

Q6:-
A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.

As in the previous exercise, select the artist_name, release, title, and year using select().
Pipe the result of this to filter() to get the tracks from the 1960s.

Solution:-



